import streamlit as st
import jieba
from collections import Counter
import matplotlib.pyplot as plt
from wordcloud import WordCloud
import re
import pandas as pd
import requests
from bs4 import BeautifulSoup
import warnings
warnings.filterwarnings("ignore")

# -------------------------- åŸºç¡€é…ç½® --------------------------
plt.rcParams["font.family"] = ["WenQuanYi Micro Hei", "Heiti TC", "DejaVu Sans"]
plt.rcParams["axes.unicode_minus"] = False

st.set_page_config(
    page_title="æ–‡æœ¬åˆ†æå·¥å…·ï¼ˆæ”¯æŒURLçˆ¬å–ï¼‰",
    page_icon="ğŸ“",
    layout="wide"
)

# -------------------------- å…¨å±€å˜é‡ï¼ˆè§£å†³ä½œç”¨åŸŸé—®é¢˜ï¼‰ --------------------------
# ç”¨session_stateå­˜å‚¨çˆ¬å–çš„æ–‡æœ¬ï¼Œé¿å…å˜é‡ä¸¢å¤±
if "crawled_text" not in st.session_state:
    st.session_state.crawled_text = ""

# -------------------------- æ ¸å¿ƒå‡½æ•° --------------------------
# 1. ç½‘é¡µå†…å®¹çˆ¬å–ï¼ˆé€‚é…HTTPåè®®+å¼ºåˆ¶æ–‡æœ¬å­˜å‚¨ï¼‰
def crawl_webpage(url):
    """çˆ¬å–æŒ‡å®šURLï¼ˆå…¼å®¹HTTP/HTTPSï¼Œå¼ºåˆ¶å­˜å‚¨åˆ°session_stateï¼‰"""
    try:
        # è¡¥å…¨URLåè®®ï¼ˆè‹¥ç”¨æˆ·åªè¾“å…¥åŸŸåï¼‰
        if not url.startswith(("http://", "https://")):
            url = "http://" + url
            st.warning(f"è‡ªåŠ¨è¡¥å…¨åè®®ï¼š{url}")
        
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
            "Accept-Language": "zh-CN,zh;q=0.9,en;q=0.8",
            "Cache-Control": "no-cache",
            "Pragma": "no-cache",
            "Upgrade-Insecure-Requests": "1"  # é€‚é…HTTPè·³è½¬HTTPS
        }
        # å¢åŠ é‡è¯•æœºåˆ¶
        for retry in range(2):
            try:
                response = requests.get(
                    url, 
                    headers=headers, 
                    timeout=20,
                    allow_redirects=True,
                    verify=False,
                    stream=False
                )
                break
            except:
                if retry == 1:
                    raise
        
        # å¼ºåˆ¶ç¼–ç é€‚é…ï¼ˆè§£å†³ä¸­æ–‡ä¹±ç ï¼‰
        encodings = ["utf-8", "gbk", "gb2312", "gb18030", response.apparent_encoding]
        content = ""
        for encoding in encodings:
            try:
                response.encoding = encoding
                content = response.text
                if content:
                    break
            except:
                continue
        
        soup = BeautifulSoup(content, "html.parser")
        for script in soup(["script", "style", "iframe", "noscript"]):
            script.decompose()
        
        # æå–æ­£æ–‡ï¼ˆé€‚é…ç›®æ ‡ç½‘ç«™çš„div/pç»“æ„ï¼‰
        content = ""
        # ä¼˜å…ˆæå–æ‰€æœ‰å¯è§æ–‡æœ¬ï¼ˆé€‚é…ç›®æ ‡ç½‘ç«™çš„æ— è§„åˆ™æ–‡æœ¬ï¼‰
        all_text = soup.get_text(separator=" ", strip=True)
        # è¿‡æ»¤è¿ç»­ç©ºç™½ç¬¦
        content = re.sub(r"\s+", " ", all_text)
        
        # å¼ºåˆ¶å­˜å‚¨åˆ°session_state
        st.session_state.crawled_text = content
        
        # é¢„è§ˆå±•ç¤º
        if len(content) < 50:
            st.warning("âš ï¸ çˆ¬å–åˆ°çš„å†…å®¹è¿‡çŸ­ï¼Œå¯èƒ½æ˜¯åçˆ¬æˆ–ç½‘é¡µæ— æœ‰æ•ˆæ–‡æœ¬")
        else:
            st.subheader("çˆ¬å–åˆ°çš„å†…å®¹é¢„è§ˆ")
            preview_text = content[:800] + "..." if len(content) > 800 else content
            st.text_area("å†…å®¹é¢„è§ˆ", preview_text, height=200, key="preview")
        
        return content
    
    except requests.exceptions.Timeout:
        st.error("âŒ è¯·æ±‚è¶…æ—¶ï¼šç½‘é¡µå“åº”æ—¶é—´è¶…è¿‡20ç§’")
        st.session_state.crawled_text = ""
        return ""
    except requests.exceptions.ConnectionError:
        st.error("âŒ è¿æ¥å¤±è´¥ï¼šæ— æ³•è®¿é—®è¯¥ç½‘å€ï¼ˆæ£€æŸ¥URLæ˜¯å¦æ­£ç¡®/ç½‘ç«™æ˜¯å¦å¯è®¿é—®ï¼‰")
        st.session_state.crawled_text = ""
        return ""
    except requests.exceptions.InvalidURL:
        st.error("âŒ æ— æ•ˆURLï¼šè¯·è¾“å…¥å®Œæ•´çš„ç½‘å€ï¼ˆç¤ºä¾‹ï¼šhttps://www.baidu.comï¼‰")
        st.session_state.crawled_text = ""
        return ""
    except Exception as e:
        st.error(f"âŒ çˆ¬å–å¤±è´¥ï¼š{str(e)[:100]}")
        st.session_state.crawled_text = ""
        return ""

# 2. æ–‡æœ¬é¢„å¤„ç†ï¼ˆé€‚é…ç›®æ ‡ç½‘ç«™çš„åŠç»“æ„åŒ–æ–‡æœ¬ï¼‰
def preprocess_text(text):
    """æ¸…æ´—æ–‡æœ¬å¹¶åˆ†è¯ï¼ˆé€‚é…å«æ•°å­—/é‡å¤å†…å®¹çš„åŠç»“æ„åŒ–æ–‡æœ¬ï¼‰"""
    # è°ƒè¯•ï¼šè¾“å‡ºsession_stateä¸­çš„æ–‡æœ¬
    st.sidebar.subheader("ğŸ” è°ƒè¯•ä¿¡æ¯")
    st.sidebar.write(f"Sessionæ–‡æœ¬é•¿åº¦ï¼š{len(text)} å­—ç¬¦")
    st.sidebar.write(f"Sessionæ–‡æœ¬å‰100å­—ç¬¦ï¼š{text[:100]}")
    
    if not text or len(text) < 10:
        st.sidebar.warning("é¢„å¤„ç†ï¼šæ–‡æœ¬è¿‡çŸ­ï¼Œè¿”å›ç©º")
        return []
    
    # ã€å…³é”®ä¿®æ”¹ï¼šä»…è¿‡æ»¤çº¯æ•°å­—/æ—¥æœŸï¼Œä¿ç•™ä¸­æ–‡è¯æ±‡ã€‘
    # æ­¥éª¤1ï¼šç§»é™¤çº¯æ•°å­—ä¸²ï¼ˆå¦‚2025-07ã€14 2012-12ç­‰ï¼‰
    text = re.sub(r"\d+[-/]\d+[-/]\d+|\d+", "", text)
    # æ­¥éª¤2ï¼šä»…ä¿ç•™ä¸­æ–‡ï¼ˆç§»é™¤æ‰€æœ‰éä¸­æ–‡å­—ç¬¦ï¼‰
    text = re.sub(r"[^\u4e00-\u9fa5]", "", text)
    # æ­¥éª¤3ï¼šç§»é™¤è¿ç»­é‡å¤çš„çŸ­æ–‡æœ¬ï¼ˆé€‚é…ç›®æ ‡ç½‘ç«™çš„é‡å¤å†…å®¹ï¼‰
    text = re.sub(r"(.{2,5})\1{3,}", r"\1", text)  # ç§»é™¤é‡å¤3æ¬¡ä»¥ä¸Šçš„2-5å­—çŸ­è¯­
    
    st.sidebar.write(f"æ¸…æ´—åæ–‡æœ¬é•¿åº¦ï¼š{len(text)} å­—ç¬¦")
    st.sidebar.write(f"æ¸…æ´—åæ–‡æœ¬å‰100å­—ç¬¦ï¼š{text[:100]}")
    
    if not text:
        st.sidebar.warning("é¢„å¤„ç†ï¼šæ¸…æ´—åæ— å†…å®¹ï¼Œè¿”å›ç©º")
        return []
    
    # åˆ†è¯ï¼ˆé€‚é…é‡å¤è¯æ±‡ï¼‰
    jieba.setLogLevel(20)
    words = jieba.lcut(text)
    
    # æç®€åœç”¨è¯è¡¨ï¼ˆä»…è¿‡æ»¤æœ€æ ¸å¿ƒï¼‰
    stop_words = {"çš„", "äº†", "æ˜¯", "åœ¨", "å’Œ", "æœ‰", "éƒ½", "è€Œ", "åŠ", "ä¸", "ä¹‹", "äº", "ä¹Ÿ", "è¿˜", "è¿™", "é‚£"}
    words = [word for word in words if word not in stop_words and len(word) >= 2]
    words = [word for word in words if word.strip()]
    
    st.sidebar.write(f"åˆ†è¯åæ•°é‡ï¼š{len(words)} ä¸ªè¯")
    st.sidebar.write(f"åˆ†è¯åå‰20ä¸ªï¼š{words[:20]}")
    
    return words

# 3. è¯äº‘ç”Ÿæˆï¼ˆå…¼å®¹äº‘ç«¯ï¼‰
def generate_wordcloud(words):
    try:
        wordcloud = WordCloud(
            width=800,
            height=400,
            background_color="white",
            font_path=None,
            max_words=100,
            colormap="viridis",
            random_state=42
        ).generate(" ".join(words))
        return wordcloud
    except Exception as e:
        st.warning(f"âš ï¸ è¯äº‘ç”Ÿæˆå¼‚å¸¸ï¼š{str(e)[:50]}ï¼Œä½¿ç”¨é™çº§æ–¹æ¡ˆ")
        wordcloud = WordCloud(
            width=800,
            height=400,
            background_color="white",
            max_words=100,
            random_state=42
        ).generate(" ".join(words))
        return wordcloud

# -------------------------- é¡µé¢äº¤äº’ --------------------------
st.title("ğŸ“ æ–‡æœ¬åˆ†æå·¥å…·ï¼ˆæ”¯æŒURLçˆ¬å–ï¼‰")
st.markdown("### æ”¯æŒï¼šç½‘é¡µå†…å®¹çˆ¬å–ã€åˆ†è¯ã€è¯é¢‘ç»Ÿè®¡ã€å…³é”®è¯æå–ã€è¯äº‘ç”Ÿæˆ")

# é€‰æ‹©è¾“å…¥æ–¹å¼
input_mode = st.radio(
    "è¯·é€‰æ‹©è¾“å…¥æ–¹å¼", 
    ["æ‰‹åŠ¨è¾“å…¥æ–‡æœ¬", "è¾“å…¥URLçˆ¬å–ç½‘é¡µå†…å®¹"], 
    horizontal=True,
    key="input_mode"
)

# æ‰‹åŠ¨è¾“å…¥æ–‡æœ¬é€»è¾‘
if input_mode == "æ‰‹åŠ¨è¾“å…¥æ–‡æœ¬":
    st.session_state.crawled_text = st.text_area(
        "è¯·è¾“å…¥éœ€è¦åˆ†æçš„æ–‡æœ¬ï¼ˆæ”¯æŒä¸­æ–‡ï¼‰",
        height=200,
        placeholder="ä¾‹å¦‚ï¼šäººå·¥æ™ºèƒ½æ˜¯æœªæ¥ç§‘æŠ€çš„æ ¸å¿ƒæ–¹å‘ï¼Œäººå·¥æ™ºèƒ½æ­£åœ¨æ”¹å˜å„è¡Œå„ä¸š...",
        key="text_input"
    )
# URLçˆ¬å–é€»è¾‘
else:
    url = st.text_input(
        "è¯·è¾“å…¥ç½‘é¡µURLï¼ˆæ”¯æŒHTTP/HTTPSï¼‰",
        placeholder="ä¾‹å¦‚ï¼šhttp://zpy.cstam.org.cn/",
        key="url_input"
    )
    if st.button("ğŸ“¤ çˆ¬å–ç½‘é¡µå†…å®¹", type="secondary", key="crawl_btn"):
        if url:
            with st.spinner("æ­£åœ¨çˆ¬å–ç½‘é¡µå†…å®¹...ï¼ˆæœ€å¤šç­‰å¾…20ç§’ï¼‰"):
                crawl_webpage(url)
        else:
            st.warning("âš ï¸ è¯·è¾“å…¥æœ‰æ•ˆçš„URLï¼")

# åˆ†ææŒ‰é’®é€»è¾‘ï¼ˆè¯»å–session_stateä¸­çš„æ–‡æœ¬ï¼‰
analyze_disabled = False if st.session_state.crawled_text.strip() else True
if st.button("ğŸš€ å¼€å§‹åˆ†æ", type="primary", key="analyze_btn", disabled=analyze_disabled):
    # ä»session_stateè¯»å–æ–‡æœ¬
    text_source = st.session_state.crawled_text
    words = preprocess_text(text_source)
    
    if not words:
        st.warning("""
        âš ï¸ æœªæå–åˆ°å¯åˆ†æçš„ä¸­æ–‡è¯æ±‡ï¼å¯èƒ½åŸå› ï¼š
        1. çˆ¬å–çš„æ–‡æœ¬ä»¥è‹±æ–‡/æ•°å­—/ç‰¹æ®Šç¬¦å·ä¸ºä¸»ï¼›
        2. æ–‡æœ¬ä¸­ä»…åŒ…å«åœç”¨è¯ï¼ˆå¦‚â€œçš„ã€äº†ã€æ˜¯â€ç­‰ï¼‰ï¼›
        3. ç½‘é¡µå†…å®¹ä¸ºå›¾ç‰‡/è§†é¢‘ï¼Œæ— æ–‡å­—ä¿¡æ¯ã€‚
        
        å»ºè®®ï¼š
        - æ›´æ¢çˆ¬å–ç›®æ ‡ï¼ˆä¼˜å…ˆé€‰æ‹©æ–°é—»ã€åšå®¢ç­‰çº¯æ–‡æœ¬ç½‘é¡µï¼‰ï¼›
        - æ‰‹åŠ¨è¡¥å……ä¸­æ–‡æ–‡æœ¬åå†åˆ†æã€‚
        """)
    else:
        word_count = Counter(words[:5000])
        top_n = word_count.most_common(min(20, len(word_count)))
        
        col1, col2 = st.columns(2)
        
        with col1:
            st.subheader(f"1. è¯é¢‘ç»Ÿè®¡ï¼ˆTOP{len(top_n)}ï¼‰")
            df = pd.DataFrame(top_n, columns=["è¯æ±‡", "å‡ºç°æ¬¡æ•°"])
            st.dataframe(df, use_container_width=True)
            
            st.subheader("2. è¯é¢‘å¯è§†åŒ–")
            if len(top_n) >= 3:
                try:
                    fig, ax = plt.subplots(figsize=(10, 6))
                    top_10 = top_n[:min(10, len(top_n))]
                    ax.bar([w[0] for w in top_10], [w[1] for w in top_10], color="#1f77b4")
                    ax.set_xlabel("è¯æ±‡", fontsize=12)
                    ax.set_ylabel("å‡ºç°æ¬¡æ•°", fontsize=12)
                    ax.set_title(f"TOP{len(top_10)}è¯æ±‡è¯é¢‘åˆ†å¸ƒ", fontsize=14)
                    plt.xticks(rotation=45)
                    st.pyplot(fig)
                except Exception as e:
                    st.error(f"âš ï¸ å›¾è¡¨ç”Ÿæˆå¤±è´¥ï¼š{str(e)[:50]}")
            else:
                st.info("âš ï¸ æœ‰æ•ˆè¯æ±‡ä¸è¶³3ä¸ªï¼Œæ— æ³•ç”ŸæˆæŸ±çŠ¶å›¾")
        
        with col2:
            st.subheader("3. æ ¸å¿ƒå…³é”®è¯")
            top_keywords = min(8, len(top_n))
            if top_keywords > 0:
                keywords = [w[0] for w in top_n[:top_keywords]]
                st.markdown(f"**{', '.join(keywords)}**")
            else:
                st.info("âš ï¸ æ— æœ‰æ•ˆå…³é”®è¯")
            
            st.subheader("4. è¯äº‘å±•ç¤º")
            try:
                wordcloud = generate_wordcloud(words)
                fig2, ax2 = plt.subplots(figsize=(10, 6))
                ax2.imshow(wordcloud, interpolation="bilinear")
                ax2.axis("off")
                st.pyplot(fig2)
            except Exception as e:
                st.error(f"âš ï¸ è¯äº‘ç”Ÿæˆå¤±è´¥ï¼š{str(e)[:50]}")

# é¡µè„š
st.divider()
st.caption("""
âœ¨ é€‚é…è¯´æ˜ï¼šé’ˆå¯¹http://zpy.cstam.org.cn/è¿™ç±»åŠç»“æ„åŒ–ç½‘é¡µåšäº†ç‰¹æ®Šé€‚é…ï¼›
âœ¨ æ ¸å¿ƒä¼˜åŒ–ï¼šè§£å†³HTTPåè®®çˆ¬å–ã€æ–‡æœ¬ä¼ é€’ä¸¢å¤±ã€é‡å¤å†…å®¹è¿‡æ»¤é—®é¢˜ï¼›
""")
