import streamlit as st
import jieba
from collections import Counter
import matplotlib.pyplot as plt
from wordcloud import WordCloud
import re
import pandas as pd
import requests
from bs4 import BeautifulSoup
import warnings
warnings.filterwarnings("ignore")

# -------------------------- åŸºç¡€é…ç½® --------------------------
# è®¾ç½®ä¸­æ–‡å­—ä½“
plt.rcParams["font.family"] = ["SimHei", "WenQuanYi Micro Hei", "Heiti TC"]
plt.rcParams["axes.unicode_minus"] = False

# é¡µé¢é…ç½®
st.set_page_config(
    page_title="æ–‡æœ¬åˆ†æå·¥å…·ï¼ˆæ”¯æŒURLçˆ¬å–ï¼‰",
    page_icon="ğŸ“",
    layout="wide"
)

# -------------------------- æ ¸å¿ƒå‡½æ•° --------------------------
# 1. ç½‘é¡µå†…å®¹çˆ¬å–å‡½æ•°
def crawl_webpage(url):
    """çˆ¬å–æŒ‡å®šURLçš„ç½‘é¡µæ–‡æœ¬å†…å®¹"""
    try:
        # è¯·æ±‚å¤´ï¼ˆæ¨¡æ‹Ÿæµè§ˆå™¨ï¼Œé¿å…è¢«åçˆ¬ï¼‰
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
        }
        # å‘é€è¯·æ±‚ï¼ˆè®¾ç½®è¶…æ—¶ï¼‰
        response = requests.get(url, headers=headers, timeout=10)
        response.encoding = response.apparent_encoding  # è‡ªåŠ¨è¯†åˆ«ç¼–ç 
        soup = BeautifulSoup(response.text, "html.parser")
        
        # æå–ç½‘é¡µä¸»è¦æ–‡æœ¬ï¼ˆä¼˜å…ˆå–æ­£æ–‡ï¼Œè¿‡æ»¤å¯¼èˆª/å¹¿å‘Šç­‰ï¼‰
        # å¸¸è§æ­£æ–‡æ ‡ç­¾ï¼špã€articleã€div[class*="content"]ã€main
        text_elements = soup.find_all(["p", "article", "main"])
        # è¡¥å……ï¼šè¿‡æ»¤class/idåŒ…å«å¹¿å‘Š/å¯¼èˆªçš„å…ƒç´ 
        text_elements = [elem for elem in text_elements if not any(
            kw in elem.get("class", []) + [elem.get("id", "")] 
            for kw in ["ad", "nav", "menu", "footer", "header"]
        )]
        
        # æå–æ–‡æœ¬å¹¶æ¸…æ´—
        raw_text = "\n".join([elem.get_text().strip() for elem in text_elements])
        if not raw_text:  # è‹¥æœªæå–åˆ°ï¼Œå–æ•´ä¸ªç½‘é¡µæ–‡æœ¬
            raw_text = soup.get_text().strip()
        
        return raw_text
    
    except requests.exceptions.Timeout:
        st.error("âŒ è¯·æ±‚è¶…æ—¶ï¼šç½‘é¡µå“åº”æ—¶é—´è¿‡é•¿")
        return ""
    except requests.exceptions.ConnectionError:
        st.error("âŒ è¿æ¥å¤±è´¥ï¼šæ— æ³•è®¿é—®è¯¥ç½‘å€ï¼ˆæ£€æŸ¥ç½‘ç»œ/ç½‘å€æ˜¯å¦æ­£ç¡®ï¼‰")
        return ""
    except requests.exceptions.InvalidURL:
        st.error("âŒ æ— æ•ˆURLï¼šè¯·è¾“å…¥å®Œæ•´çš„ç½‘å€ï¼ˆå¦‚ https://www.xxx.comï¼‰")
        return ""
    except Exception as e:
        st.error(f"âŒ çˆ¬å–å¤±è´¥ï¼š{str(e)}")
        return ""

# 2. æ–‡æœ¬é¢„å¤„ç†å‡½æ•°
def preprocess_text(text):
    """æ¸…æ´—æ–‡æœ¬å¹¶åˆ†è¯"""
    # å»é™¤æ ‡ç‚¹ã€æ•°å­—ã€ç©ºç™½ç¬¦ã€ç‰¹æ®Šç¬¦å·
    text = re.sub(r"[^\u4e00-\u9fa5a-zA-Z]", "", text)
    # åˆ†è¯
    words = jieba.lcut(text)
    # è¿‡æ»¤åœç”¨è¯å’Œå•å­—
    stop_words = {"çš„", "äº†", "æ˜¯", "åœ¨", "å’Œ", "æœ‰", "æˆ‘", "ä½ ", "ä»–", "å¥¹", "å®ƒ", 
                  "å°±", "éƒ½", "è€Œ", "åŠ", "ä¸", "ä¹‹", "äº", "ä¹Ÿ", "è¿˜", "è¿™", "é‚£",
                  "ä¸º", "ä»¥", "å¯", "å°†", "å¯¹", "èƒ½", "ä¼š", "è¦", "æŠŠ", "è¢«", "æ‰€"}
    words = [word for word in words if word not in stop_words and len(word) > 1]
    return words

# -------------------------- é¡µé¢äº¤äº’ --------------------------
st.title("ğŸ“ æ–‡æœ¬åˆ†æå·¥å…·ï¼ˆæ”¯æŒURLçˆ¬å–ï¼‰")
st.markdown("### æ”¯æŒï¼šç½‘é¡µå†…å®¹çˆ¬å–ã€åˆ†è¯ã€è¯é¢‘ç»Ÿè®¡ã€å…³é”®è¯æå–ã€è¯äº‘ç”Ÿæˆ")

# é€‰æ‹©è¾“å…¥æ–¹å¼
input_mode = st.radio("è¯·é€‰æ‹©è¾“å…¥æ–¹å¼", ["æ‰‹åŠ¨è¾“å…¥æ–‡æœ¬", "è¾“å…¥URLçˆ¬å–ç½‘é¡µå†…å®¹"], horizontal=True)

text_source = ""
if input_mode == "æ‰‹åŠ¨è¾“å…¥æ–‡æœ¬":
    # æ‰‹åŠ¨è¾“å…¥æ–‡æœ¬
    text_source = st.text_area(
        "è¯·è¾“å…¥éœ€è¦åˆ†æçš„æ–‡æœ¬ï¼ˆæ”¯æŒä¸­æ–‡ï¼‰",
        height=200,
        placeholder="ä¾‹å¦‚ï¼šäººå·¥æ™ºèƒ½æ˜¯æœªæ¥ç§‘æŠ€çš„æ ¸å¿ƒæ–¹å‘ï¼Œäººå·¥æ™ºèƒ½æ­£åœ¨æ”¹å˜å„è¡Œå„ä¸š..."
    )
else:
    # URLè¾“å…¥
    url = st.text_input(
        "è¯·è¾“å…¥ç½‘é¡µURLï¼ˆéœ€åŒ…å« https://ï¼‰",
        placeholder="ä¾‹å¦‚ï¼šhttps://www.baidu.com/news/xxx.html"
    )
    # çˆ¬å–æŒ‰é’®
    if st.button("ğŸ“¤ çˆ¬å–ç½‘é¡µå†…å®¹", type="secondary"):
        if url:
            with st.spinner("æ­£åœ¨çˆ¬å–ç½‘é¡µå†…å®¹..."):
                text_source = crawl_webpage(url)
                # å±•ç¤ºçˆ¬å–ç»“æœï¼ˆä¾›ç¡®è®¤ï¼‰
                st.subheader("çˆ¬å–åˆ°çš„å†…å®¹é¢„è§ˆ")
                st.text_area("å†…å®¹é¢„è§ˆï¼ˆå‰500å­—ï¼‰", text_source[:500], height=150)
        else:
            st.warning("âš ï¸ è¯·è¾“å…¥æœ‰æ•ˆçš„URLï¼")

# åˆ†ææŒ‰é’®
if st.button("ğŸš€ å¼€å§‹åˆ†æ", type="primary"):
    if not text_source.strip():
        st.warning("âš ï¸ è¯·å…ˆè¾“å…¥æ–‡æœ¬/çˆ¬å–ç½‘é¡µå†…å®¹ï¼")
    else:
        # é¢„å¤„ç†
        words = preprocess_text(text_source)
        if not words:
            st.warning("âš ï¸ æ— æœ‰æ•ˆåˆ†æå†…å®¹ï¼ˆè¯·æ£€æŸ¥æ–‡æœ¬æ˜¯å¦åŒ…å«ä¸­æ–‡è¯æ±‡ï¼‰ï¼")
        else:
            # è¯é¢‘ç»Ÿè®¡
            word_count = Counter(words)
            top_20 = word_count.most_common(20)
            
            # åˆ†æ å±•ç¤ºç»“æœ
            col1, col2 = st.columns(2)
            
            with col1:
                st.subheader("1. è¯é¢‘ç»Ÿè®¡ï¼ˆTOP20ï¼‰")
                df = pd.DataFrame(top_20, columns=["è¯æ±‡", "å‡ºç°æ¬¡æ•°"])
                st.dataframe(df, use_container_width=True)
                
                # è¯é¢‘æŸ±çŠ¶å›¾
                st.subheader("2. è¯é¢‘å¯è§†åŒ–ï¼ˆTOP10ï¼‰")
                fig, ax = plt.subplots(figsize=(10, 6))
                ax.bar(df["è¯æ±‡"][:10], df["å‡ºç°æ¬¡æ•°"][:10], color="#1f77b4")
                ax.set_xlabel("è¯æ±‡", fontsize=12)
                ax.set_ylabel("å‡ºç°æ¬¡æ•°", fontsize=12)
                ax.set_title("TOP10è¯æ±‡è¯é¢‘åˆ†å¸ƒ", fontsize=14)
                plt.xticks(rotation=45)
                st.pyplot(fig)
            
            with col2:
                st.subheader("3. æ ¸å¿ƒå…³é”®è¯ï¼ˆTOP8ï¼‰")
                keywords = [w[0] for w in top_20[:8]]
                st.markdown(f"**{', '.join(keywords)}**")
                
                # è¯äº‘ç”Ÿæˆ
                st.subheader("4. è¯äº‘å±•ç¤º")
                wordcloud = WordCloud(
                    width=800,
                    height=400,
                    background_color="white",
                    font_path="simhei.ttf",  # å…¼å®¹äº‘ç«¯ä¸­æ–‡æ˜¾ç¤º
                    max_words=100,
                    colormap="viridis"
                ).generate(" ".join(words))
                fig2, ax2 = plt.subplots(figsize=(10, 6))
                ax2.imshow(wordcloud, interpolation="bilinear")
                ax2.axis("off")
                st.pyplot(fig2)

# é¡µè„š
st.divider()
st.caption("âœ¨ æ³¨æ„ï¼šéƒ¨åˆ†ç½‘ç«™æœ‰åçˆ¬æœºåˆ¶ï¼Œå¯èƒ½æ— æ³•çˆ¬å–å†…å®¹ | åŸºäºStreamlit+BeautifulSoupå¼€å‘")
