import streamlit as st
import jieba
from collections import Counter
import matplotlib.pyplot as plt
from wordcloud import WordCloud
import re
import pandas as pd
import requests
from bs4 import BeautifulSoup
import warnings
warnings.filterwarnings("ignore")

# -------------------------- åŸºç¡€é…ç½® --------------------------
# è®¾ç½®ä¸­æ–‡å­—ä½“ï¼ˆå…¼å®¹äº‘ç«¯ï¼‰
plt.rcParams["font.family"] = ["WenQuanYi Micro Hei", "Heiti TC", "DejaVu Sans"]
plt.rcParams["axes.unicode_minus"] = False

# é¡µé¢é…ç½®
st.set_page_config(
    page_title="æ–‡æœ¬åˆ†æå·¥å…·ï¼ˆæ”¯æŒURLçˆ¬å–ï¼‰",
    page_icon="ğŸ“",
    layout="wide"
)

# -------------------------- æ ¸å¿ƒå‡½æ•° --------------------------
# 1. ç½‘é¡µå†…å®¹çˆ¬å–+å¢å¼ºæ¸…æ´—
def crawl_webpage(url):
    """çˆ¬å–æŒ‡å®šURLçš„ç½‘é¡µæ–‡æœ¬å†…å®¹ï¼ˆå¢å¼ºç‰ˆï¼‰"""
    try:
        # å¼ºåŒ–è¯·æ±‚å¤´
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
            "Accept-Language": "zh-CN,zh;q=0.9,en;q=0.8",
            "Cache-Control": "no-cache",
            "Pragma": "no-cache"
        }
        # å‘é€è¯·æ±‚ï¼ˆå…è®¸é‡å®šå‘ï¼‰
        response = requests.get(
            url, 
            headers=headers, 
            timeout=15,
            allow_redirects=True,
            verify=False  # å¿½ç•¥SSLè¯ä¹¦é”™è¯¯
        )
        # å¼ºåˆ¶æŒ‡å®šç¼–ç ï¼ˆä¼˜å…ˆutf-8ï¼Œå…œåº•gbkï¼‰
        try:
            response.encoding = "utf-8"
        except:
            response.encoding = response.apparent_encoding or "gbk"
        
        soup = BeautifulSoup(response.text, "html.parser")
        
        # ç§»é™¤æ— å…³æ ‡ç­¾ï¼ˆè„šæœ¬ã€æ ·å¼ã€å¹¿å‘Šï¼‰
        for script in soup(["script", "style", "iframe", "noscript"]):
            script.decompose()
        
        # ç²¾å‡†æå–æ­£æ–‡ï¼ˆå¤šè§„åˆ™åŒ¹é…ï¼‰
        content = ""
        # è§„åˆ™1ï¼šåŒ¹é…å¸¸è§æ­£æ–‡class/id
        content_tags = soup.find_all(
            "div", 
            class_=re.compile(r"content|article|main|text|body|detail", re.I)
        ) or soup.find_all("article") or soup.find_all("main")
        if content_tags:
            content = "\n".join([tag.get_text().strip() for tag in content_tags])
        # è§„åˆ™2ï¼šè‹¥æœªåŒ¹é…ï¼Œå–æ‰€æœ‰pæ ‡ç­¾
        if not content:
            p_tags = soup.find_all("p")
            content = "\n".join([p.get_text().strip() for p in p_tags])
        # è§„åˆ™3ï¼šç»ˆæå…œåº•ï¼ˆå…¨æ–‡æœ¬ï¼‰
        if not content:
            content = soup.get_text().strip()
        
        # æ¸…æ´—çˆ¬å–çš„æ–‡æœ¬ï¼ˆå»é‡ç©ºè¡Œã€å¤šä½™ç©ºæ ¼ï¼‰
        content = re.sub(r"\n+", "\n", content)
        content = re.sub(r"\s+", " ", content)
        
        if len(content) < 50:  # è¿‡æ»¤æ— æ•ˆå†…å®¹
            st.warning("âš ï¸ çˆ¬å–åˆ°çš„å†…å®¹è¿‡çŸ­ï¼Œå¯èƒ½æ˜¯åçˆ¬æˆ–ç½‘é¡µæ— æœ‰æ•ˆæ–‡æœ¬")
            return ""
        
        return content
    
    except requests.exceptions.Timeout:
        st.error("âŒ è¯·æ±‚è¶…æ—¶ï¼šç½‘é¡µå“åº”æ—¶é—´è¶…è¿‡15ç§’")
        return ""
    except requests.exceptions.ConnectionError:
        st.error("âŒ è¿æ¥å¤±è´¥ï¼šæ— æ³•è®¿é—®è¯¥ç½‘å€ï¼ˆæ£€æŸ¥URLæ˜¯å¦æ­£ç¡®/ç½‘ç«™æ˜¯å¦å¯è®¿é—®ï¼‰")
        return ""
    except requests.exceptions.InvalidURL:
        st.error("âŒ æ— æ•ˆURLï¼šè¯·è¾“å…¥å®Œæ•´çš„ç½‘å€ï¼ˆç¤ºä¾‹ï¼šhttps://www.baidu.comï¼‰")
        return ""
    except Exception as e:
        st.error(f"âŒ çˆ¬å–å¤±è´¥ï¼š{str(e)[:100]}")  # æˆªæ–­è¿‡é•¿é”™è¯¯ä¿¡æ¯
        return ""

# 2. æ–‡æœ¬é¢„å¤„ç†+å®¹é”™
def preprocess_text(text):
    """æ¸…æ´—æ–‡æœ¬å¹¶åˆ†è¯ï¼ˆå¢å¼ºå®¹é”™ï¼‰"""
    if not text or len(text) < 10:
        return []
    
    # ä¿ç•™ä¸­æ–‡ï¼Œç§»é™¤æ‰€æœ‰éä¸­æ–‡å­—ç¬¦
    text = re.sub(r"[^\u4e00-\u9fa5]", "", text)
    if not text:
        return []
    
    # åˆ†è¯ï¼ˆå…³é—­jiebaæ—¥å¿—ï¼‰
    jieba.setLogLevel(20)
    words = jieba.lcut(text)
    
    # æ‰©å±•åœç”¨è¯è¡¨+è¿‡æ»¤
    stop_words = {
        "çš„", "äº†", "æ˜¯", "åœ¨", "å’Œ", "æœ‰", "æˆ‘", "ä½ ", "ä»–", "å¥¹", "å®ƒ", 
        "å°±", "éƒ½", "è€Œ", "åŠ", "ä¸", "ä¹‹", "äº", "ä¹Ÿ", "è¿˜", "è¿™", "é‚£",
        "ä¸º", "ä»¥", "å¯", "å°†", "å¯¹", "èƒ½", "ä¼š", "è¦", "æŠŠ", "è¢«", "æ‰€",
        "è¯¥", "ä»", "åˆ°", "å› ", "ç”±", "éš", "å¦‚", "è‹¥", "åˆ™", "æˆ–", "å³",
        "ç€", "è¿‡", "å‘¢", "å—", "å§", "å•Š", "å“¦", "å—¯", "å“ˆ", "å“", "å“Ÿ",
        "ä¸€", "äºŒ", "ä¸‰", "å››", "äº”", "å…­", "ä¸ƒ", "å…«", "ä¹", "å", "ç™¾", "åƒ", "ä¸‡"
    }
    # è¿‡æ»¤åœç”¨è¯ã€å•å­—ã€è¿‡çŸ­è¯æ±‡
    words = [word for word in words if word not in stop_words and len(word) >= 2]
    
    # å»é‡åå†ç»Ÿè®¡ï¼ˆé¿å…é‡å¤è¯æ±‡å¹²æ‰°ï¼‰
    words = list(filter(None, words))
    return words

# 3. è¯äº‘ç”Ÿæˆå®¹é”™å‡½æ•°
def generate_wordcloud(words):
    """ç”Ÿæˆè¯äº‘ï¼ˆå…¼å®¹äº‘ç«¯æ— æœ¬åœ°å­—ä½“ï¼‰"""
    try:
        # ä¼˜å…ˆä½¿ç”¨ç³»ç»Ÿå†…ç½®ä¸­æ–‡å­—ä½“ï¼Œé¿å…æœ¬åœ°å­—ä½“ä¾èµ–
        wordcloud = WordCloud(
            width=800,
            height=400,
            background_color="white",
            font_path=None,  # äº‘ç«¯è‡ªåŠ¨åŒ¹é…å­—ä½“
            max_words=100,
            colormap="viridis",
            random_state=42  # å›ºå®šéšæœºç§å­ï¼Œç»“æœç¨³å®š
        ).generate(" ".join(words))
        return wordcloud
    except Exception as e:
        # é™çº§æ–¹æ¡ˆï¼šä½¿ç”¨é»˜è®¤å­—ä½“+æç¤º
        st.warning(f"âš ï¸ è¯äº‘ç”Ÿæˆå¼‚å¸¸ï¼š{str(e)[:50]}ï¼Œä½¿ç”¨é™çº§æ–¹æ¡ˆ")
        wordcloud = WordCloud(
            width=800,
            height=400,
            background_color="white",
            max_words=100,
            random_state=42
        ).generate(" ".join(words))
        return wordcloud

# -------------------------- é¡µé¢äº¤äº’ --------------------------
st.title("ğŸ“ æ–‡æœ¬åˆ†æå·¥å…·ï¼ˆæ”¯æŒURLçˆ¬å–ï¼‰")
st.markdown("### æ”¯æŒï¼šç½‘é¡µå†…å®¹çˆ¬å–ã€åˆ†è¯ã€è¯é¢‘ç»Ÿè®¡ã€å…³é”®è¯æå–ã€è¯äº‘ç”Ÿæˆ")

# é€‰æ‹©è¾“å…¥æ–¹å¼
input_mode = st.radio(
    "è¯·é€‰æ‹©è¾“å…¥æ–¹å¼", 
    ["æ‰‹åŠ¨è¾“å…¥æ–‡æœ¬", "è¾“å…¥URLçˆ¬å–ç½‘é¡µå†…å®¹"], 
    horizontal=True,
    key="input_mode"  # å”¯ä¸€keyï¼Œé¿å…çŠ¶æ€å¼‚å¸¸
)

text_source = ""
if input_mode == "æ‰‹åŠ¨è¾“å…¥æ–‡æœ¬":
    # æ‰‹åŠ¨è¾“å…¥æ–‡æœ¬
    text_source = st.text_area(
        "è¯·è¾“å…¥éœ€è¦åˆ†æçš„æ–‡æœ¬ï¼ˆæ”¯æŒä¸­æ–‡ï¼‰",
        height=200,
        placeholder="ä¾‹å¦‚ï¼šäººå·¥æ™ºèƒ½æ˜¯æœªæ¥ç§‘æŠ€çš„æ ¸å¿ƒæ–¹å‘ï¼Œäººå·¥æ™ºèƒ½æ­£åœ¨æ”¹å˜å„è¡Œå„ä¸š...",
        key="text_input"
    )
else:
    # URLè¾“å…¥
    url = st.text_input(
        "è¯·è¾“å…¥ç½‘é¡µURLï¼ˆéœ€åŒ…å« https://ï¼‰",
        placeholder="ä¾‹å¦‚ï¼šhttps://news.sina.com.cn/c/2025-01-01/doc-xxxx.shtml",
        key="url_input"
    )
    # çˆ¬å–æŒ‰é’®
    if st.button("ğŸ“¤ çˆ¬å–ç½‘é¡µå†…å®¹", type="secondary", key="crawl_btn"):
        if url:
            with st.spinner("æ­£åœ¨çˆ¬å–ç½‘é¡µå†…å®¹...ï¼ˆè¯·ç¨å€™ï¼Œæœ€å¤šç­‰å¾…15ç§’ï¼‰"):
                text_source = crawl_webpage(url)
                # å±•ç¤ºçˆ¬å–ç»“æœï¼ˆä¾›ç¡®è®¤ï¼‰
                if text_source:
                    st.subheader("çˆ¬å–åˆ°çš„å†…å®¹é¢„è§ˆ")
                    preview_text = text_source[:800] + "..." if len(text_source) > 800 else text_source
                    st.text_area("å†…å®¹é¢„è§ˆ", preview_text, height=200, key="preview")
        else:
            st.warning("âš ï¸ è¯·è¾“å…¥æœ‰æ•ˆçš„URLï¼")

# åˆ†ææŒ‰é’®ï¼ˆå¢åŠ é˜²é‡å¤ç‚¹å‡»ï¼‰
if st.button("ğŸš€ å¼€å§‹åˆ†æ", type="primary", key="analyze_btn", disabled=not text_source.strip()):
    # é¢„å¤„ç†
    words = preprocess_text(text_source)
    if not words:
        st.warning("âš ï¸ æ— æœ‰æ•ˆåˆ†æå†…å®¹ï¼å¯èƒ½åŸå› ï¼š\n1. æ–‡æœ¬ä»…å«æ— æ„ä¹‰å­—ç¬¦\n2. çˆ¬å–å†…å®¹è¢«åçˆ¬è¿‡æ»¤\n3. æ–‡æœ¬æ— ä¸­æ–‡è¯æ±‡")
    else:
        # è¯é¢‘ç»Ÿè®¡ï¼ˆé™åˆ¶æœ€å¤§æ•°é‡ï¼Œé¿å…å¡é¡¿ï¼‰
        word_count = Counter(words[:5000])  # ä»…å–å‰5000ä¸ªè¯ï¼Œé˜²æ­¢å†…å­˜æº¢å‡º
        top_20 = word_count.most_common(20)
        
        # åˆ†æ å±•ç¤ºç»“æœ
        col1, col2 = st.columns(2)
        
        with col1:
            st.subheader("1. è¯é¢‘ç»Ÿè®¡ï¼ˆTOP20ï¼‰")
            df = pd.DataFrame(top_20, columns=["è¯æ±‡", "å‡ºç°æ¬¡æ•°"])
            st.dataframe(df, use_container_width=True)
            
            # è¯é¢‘æŸ±çŠ¶å›¾ï¼ˆå®¹é”™ï¼‰
            st.subheader("2. è¯é¢‘å¯è§†åŒ–ï¼ˆTOP10ï¼‰")
            try:
                fig, ax = plt.subplots(figsize=(10, 6))
                ax.bar(df["è¯æ±‡"][:10], df["å‡ºç°æ¬¡æ•°"][:10], color="#1f77b4")
                ax.set_xlabel("è¯æ±‡", fontsize=12)
                ax.set_ylabel("å‡ºç°æ¬¡æ•°", fontsize=12)
                ax.set_title("TOP10è¯æ±‡è¯é¢‘åˆ†å¸ƒ", fontsize=14)
                plt.xticks(rotation=45)
                st.pyplot(fig)
            except Exception as e:
                st.error(f"âš ï¸ å›¾è¡¨ç”Ÿæˆå¤±è´¥ï¼š{str(e)[:50]}")
        
        with col2:
            st.subheader("3. æ ¸å¿ƒå…³é”®è¯ï¼ˆTOP8ï¼‰")
            keywords = [w[0] for w in top_20[:8]] if top_20 else []
            if keywords:
                st.markdown(f"**{', '.join(keywords)}**")
            else:
                st.write("æš‚æ— æœ‰æ•ˆå…³é”®è¯")
            
            # è¯äº‘å±•ç¤ºï¼ˆå®¹é”™ï¼‰
            st.subheader("4. è¯äº‘å±•ç¤º")
            try:
                wordcloud = generate_wordcloud(words)
                fig2, ax2 = plt.subplots(figsize=(10, 6))
                ax2.imshow(wordcloud, interpolation="bilinear")
                ax2.axis("off")
                st.pyplot(fig2)
            except Exception as e:
                st.error(f"âš ï¸ è¯äº‘ç”Ÿæˆå¤±è´¥ï¼š{str(e)[:50]}")

# é¡µè„š
st.divider()
st.caption("""
âœ¨ æ³¨æ„äº‹é¡¹ï¼š
1. éƒ¨åˆ†ç½‘ç«™ï¼ˆå¦‚çŸ¥ä¹ã€æ·˜å®ï¼‰æœ‰ä¸¥æ ¼åçˆ¬æœºåˆ¶ï¼Œå¯èƒ½æ— æ³•çˆ¬å–ï¼›
2. å»ºè®®çˆ¬å–æ–°é—»ã€åšå®¢ã€ç™¾ç§‘ç­‰é™æ€ç½‘é¡µï¼›
3. è‹¥åˆ†æå¤±è´¥ï¼Œè¯·æ£€æŸ¥æ–‡æœ¬æ˜¯å¦åŒ…å«è¶³å¤Ÿçš„ä¸­æ–‡å†…å®¹ã€‚
""")
