import streamlit as st
import jieba
from collections import Counter
import matplotlib.pyplot as plt
from wordcloud import WordCloud
import re
import pandas as pd
import requests
from bs4 import BeautifulSoup
import warnings
warnings.filterwarnings("ignore")

# -------------------------- åŸºç¡€é…ç½® --------------------------
plt.rcParams["font.family"] = ["WenQuanYi Micro Hei", "Heiti TC", "DejaVu Sans"]
plt.rcParams["axes.unicode_minus"] = False

st.set_page_config(
    page_title="æ–‡æœ¬åˆ†æå·¥å…·ï¼ˆæ”¯æŒURLçˆ¬å–ï¼‰",
    page_icon="ğŸ“",
    layout="wide"
)

# -------------------------- æ ¸å¿ƒå‡½æ•° --------------------------
# 1. ç½‘é¡µå†…å®¹çˆ¬å–ï¼ˆä¿ç•™åŸæœ‰é€»è¾‘ï¼Œä»…ä¼˜åŒ–è¿”å›æç¤ºï¼‰
def crawl_webpage(url):
    try:
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
            "Accept-Language": "zh-CN,zh;q=0.9,en;q=0.8",
            "Cache-Control": "no-cache",
            "Pragma": "no-cache"
        }
        response = requests.get(
            url, 
            headers=headers, 
            timeout=15,
            allow_redirects=True,
            verify=False
        )
        try:
            response.encoding = "utf-8"
        except:
            response.encoding = response.apparent_encoding or "gbk"
        
        soup = BeautifulSoup(response.text, "html.parser")
        for script in soup(["script", "style", "iframe", "noscript"]):
            script.decompose()
        
        # ç²¾å‡†æå–æ­£æ–‡
        content = ""
        content_tags = soup.find_all(
            "div", 
            class_=re.compile(r"content|article|main|text|body|detail", re.I)
        ) or soup.find_all("article") or soup.find_all("main")
        if content_tags:
            content = "\n".join([tag.get_text().strip() for tag in content_tags])
        if not content:
            p_tags = soup.find_all("p")
            content = "\n".join([p.get_text().strip() for p in p_tags])
        if not content:
            content = soup.get_text().strip()
        
        content = re.sub(r"\n+", "\n", content)
        content = re.sub(r"\s+", " ", content)
        
        if len(content) < 50:
            st.warning("âš ï¸ çˆ¬å–åˆ°çš„å†…å®¹è¿‡çŸ­ï¼Œå¯èƒ½æ˜¯åçˆ¬æˆ–ç½‘é¡µæ— æœ‰æ•ˆæ–‡æœ¬")
            return ""
        
        return content
    
    except requests.exceptions.Timeout:
        st.error("âŒ è¯·æ±‚è¶…æ—¶ï¼šç½‘é¡µå“åº”æ—¶é—´è¶…è¿‡15ç§’")
        return ""
    except requests.exceptions.ConnectionError:
        st.error("âŒ è¿æ¥å¤±è´¥ï¼šæ— æ³•è®¿é—®è¯¥ç½‘å€ï¼ˆæ£€æŸ¥URLæ˜¯å¦æ­£ç¡®/ç½‘ç«™æ˜¯å¦å¯è®¿é—®ï¼‰")
        return ""
    except requests.exceptions.InvalidURL:
        st.error("âŒ æ— æ•ˆURLï¼šè¯·è¾“å…¥å®Œæ•´çš„ç½‘å€ï¼ˆç¤ºä¾‹ï¼šhttps://www.baidu.comï¼‰")
        return ""
    except Exception as e:
        st.error(f"âŒ çˆ¬å–å¤±è´¥ï¼š{str(e)[:100]}")
        return ""

# 2. æ–‡æœ¬é¢„å¤„ç†ã€é‡ç‚¹ä¿®æ”¹ï¼šæ”¾å®½è¿‡æ»¤è§„åˆ™+ä¿ç•™æ—¥å¿—ã€‘
def preprocess_text(text):
    """æ¸…æ´—æ–‡æœ¬å¹¶åˆ†è¯ï¼ˆæ”¾å®½è¿‡æ»¤è§„åˆ™ï¼Œå¢åŠ è°ƒè¯•ä¿¡æ¯ï¼‰"""
    # è°ƒè¯•ï¼šè¾“å‡ºåŸå§‹æ–‡æœ¬é•¿åº¦å’Œå‰100å­—ç¬¦
    st.sidebar.subheader("ğŸ” è°ƒè¯•ä¿¡æ¯")
    st.sidebar.write(f"åŸå§‹æ–‡æœ¬é•¿åº¦ï¼š{len(text)} å­—ç¬¦")
    st.sidebar.write(f"åŸå§‹æ–‡æœ¬å‰100å­—ç¬¦ï¼š{text[:100]}")
    
    if not text or len(text) < 10:
        st.sidebar.warning("é¢„å¤„ç†ï¼šæ–‡æœ¬è¿‡çŸ­ï¼Œè¿”å›ç©º")
        return []
    
    # ã€ä¿®æ”¹1ï¼šä¿ç•™ä¸­æ–‡+ä¸­æ–‡æ ‡ç‚¹ï¼Œä¸å†å®Œå…¨ç§»é™¤éä¸­æ–‡ã€‘
    # åªç§»é™¤è‹±æ–‡ã€æ•°å­—ã€ç‰¹æ®Šç¬¦å·ï¼Œä¿ç•™ä¸­æ–‡å’Œä¸­æ–‡æ ‡ç‚¹
    text = re.sub(r"[a-zA-Z0-9`~!@#$%^&*()_+-=<>?/:;\"\'\\|{}[\]Â·~ï¼@#ï¿¥%â€¦â€¦&*ï¼ˆï¼‰â€”â€”+-=ã€Šã€‹ï¼Ÿï¼šï¼›â€œâ€â€˜â€™ã€|{}ã€ã€‘]", "", text)
    st.sidebar.write(f"æ¸…æ´—åæ–‡æœ¬é•¿åº¦ï¼š{len(text)} å­—ç¬¦")
    st.sidebar.write(f"æ¸…æ´—åæ–‡æœ¬å‰100å­—ç¬¦ï¼š{text[:100]}")
    
    if not text:
        st.sidebar.warning("é¢„å¤„ç†ï¼šæ¸…æ´—åæ— å†…å®¹ï¼Œè¿”å›ç©º")
        return []
    
    # åˆ†è¯
    jieba.setLogLevel(20)
    words = jieba.lcut(text)
    st.sidebar.write(f"åˆ†è¯ç»“æœæ•°é‡ï¼š{len(words)} ä¸ªè¯")
    st.sidebar.write(f"åˆ†è¯ç»“æœå‰20ä¸ªï¼š{words[:20]}")
    
    # ã€ä¿®æ”¹2ï¼šç¼©å‡åœç”¨è¯è¡¨ï¼Œä»…ä¿ç•™æœ€æ ¸å¿ƒåœç”¨è¯ã€‘
    stop_words = {
        "çš„", "äº†", "æ˜¯", "åœ¨", "å’Œ", "æœ‰", "æˆ‘", "ä½ ", "ä»–", "éƒ½", "è€Œ", "åŠ", "ä¸", "ä¹‹", "äº", "ä¹Ÿ", "è¿˜", "è¿™", "é‚£"
    }
    # ã€ä¿®æ”¹3ï¼šä»…è¿‡æ»¤åœç”¨è¯ï¼Œä¸å†è¿‡æ»¤å•å­—ï¼ˆä¿ç•™çŸ­è¯æ±‡ï¼‰ã€‘
    words = [word for word in words if word not in stop_words]
    # è¿‡æ»¤ç©ºå­—ç¬¦ä¸²
    words = [word for word in words if word.strip()]
    
    st.sidebar.write(f"è¿‡æ»¤åœç”¨è¯åæ•°é‡ï¼š{len(words)} ä¸ªè¯")
    st.sidebar.write(f"è¿‡æ»¤åå‰20ä¸ªï¼š{words[:20]}")
    
    return words

# 3. è¯äº‘ç”Ÿæˆï¼ˆå…¼å®¹äº‘ç«¯ï¼‰
def generate_wordcloud(words):
    try:
        wordcloud = WordCloud(
            width=800,
            height=400,
            background_color="white",
            font_path=None,
            max_words=100,
            colormap="viridis",
            random_state=42
        ).generate(" ".join(words))
        return wordcloud
    except Exception as e:
        st.warning(f"âš ï¸ è¯äº‘ç”Ÿæˆå¼‚å¸¸ï¼š{str(e)[:50]}ï¼Œä½¿ç”¨é™çº§æ–¹æ¡ˆ")
        wordcloud = WordCloud(
            width=800,
            height=400,
            background_color="white",
            max_words=100,
            random_state=42
        ).generate(" ".join(words))
        return wordcloud

# -------------------------- é¡µé¢äº¤äº’ --------------------------
st.title("ğŸ“ æ–‡æœ¬åˆ†æå·¥å…·ï¼ˆæ”¯æŒURLçˆ¬å–ï¼‰")
st.markdown("### æ”¯æŒï¼šç½‘é¡µå†…å®¹çˆ¬å–ã€åˆ†è¯ã€è¯é¢‘ç»Ÿè®¡ã€å…³é”®è¯æå–ã€è¯äº‘ç”Ÿæˆ")

# é€‰æ‹©è¾“å…¥æ–¹å¼
input_mode = st.radio(
    "è¯·é€‰æ‹©è¾“å…¥æ–¹å¼", 
    ["æ‰‹åŠ¨è¾“å…¥æ–‡æœ¬", "è¾“å…¥URLçˆ¬å–ç½‘é¡µå†…å®¹"], 
    horizontal=True,
    key="input_mode"
)

text_source = ""
if input_mode == "æ‰‹åŠ¨è¾“å…¥æ–‡æœ¬":
    text_source = st.text_area(
        "è¯·è¾“å…¥éœ€è¦åˆ†æçš„æ–‡æœ¬ï¼ˆæ”¯æŒä¸­æ–‡ï¼‰",
        height=200,
        placeholder="ä¾‹å¦‚ï¼šäººå·¥æ™ºèƒ½æ˜¯æœªæ¥ç§‘æŠ€çš„æ ¸å¿ƒæ–¹å‘ï¼Œäººå·¥æ™ºèƒ½æ­£åœ¨æ”¹å˜å„è¡Œå„ä¸š...",
        key="text_input"
    )
else:
    url = st.text_input(
        "è¯·è¾“å…¥ç½‘é¡µURLï¼ˆéœ€åŒ…å« https://ï¼‰",
        placeholder="ä¾‹å¦‚ï¼šhttps://news.sina.com.cn/c/2025-01-01/doc-xxxx.shtml",
        key="url_input"
    )
    if st.button("ğŸ“¤ çˆ¬å–ç½‘é¡µå†…å®¹", type="secondary", key="crawl_btn"):
        if url:
            with st.spinner("æ­£åœ¨çˆ¬å–ç½‘é¡µå†…å®¹...ï¼ˆè¯·ç¨å€™ï¼Œæœ€å¤šç­‰å¾…15ç§’ï¼‰"):
                text_source = crawl_webpage(url)
                if text_source:
                    st.subheader("çˆ¬å–åˆ°çš„å†…å®¹é¢„è§ˆ")
                    preview_text = text_source[:800] + "..." if len(text_source) > 800 else text_source
                    st.text_area("å†…å®¹é¢„è§ˆ", preview_text, height=200, key="preview")
        else:
            st.warning("âš ï¸ è¯·è¾“å…¥æœ‰æ•ˆçš„URLï¼")

# åˆ†ææŒ‰é’®ï¼ˆä¼˜åŒ–ç¦ç”¨é€»è¾‘ï¼‰
analyze_disabled = False if text_source.strip() else True
if st.button("ğŸš€ å¼€å§‹åˆ†æ", type="primary", key="analyze_btn", disabled=analyze_disabled):
    # é¢„å¤„ç†
    words = preprocess_text(text_source)
    
    # ã€æ–°å¢ï¼šå…œåº•é€»è¾‘ï¼Œæ— æœ‰æ•ˆè¯æ±‡æ—¶ç»™å‡ºå¼•å¯¼ã€‘
    if not words:
        st.warning("""
        âš ï¸ æœªæå–åˆ°å¯åˆ†æçš„ä¸­æ–‡è¯æ±‡ï¼å¯èƒ½åŸå› ï¼š
        1. çˆ¬å–çš„æ–‡æœ¬ä»¥è‹±æ–‡/æ•°å­—/ç‰¹æ®Šç¬¦å·ä¸ºä¸»ï¼›
        2. æ–‡æœ¬ä¸­ä»…åŒ…å«åœç”¨è¯ï¼ˆå¦‚â€œçš„ã€äº†ã€æ˜¯â€ç­‰ï¼‰ï¼›
        3. ç½‘é¡µå†…å®¹ä¸ºå›¾ç‰‡/è§†é¢‘ï¼Œæ— æ–‡å­—ä¿¡æ¯ã€‚
        
        å»ºè®®ï¼š
        - æ›´æ¢çˆ¬å–ç›®æ ‡ï¼ˆä¼˜å…ˆé€‰æ‹©æ–°é—»ã€åšå®¢ç­‰çº¯æ–‡æœ¬ç½‘é¡µï¼‰ï¼›
        - æ‰‹åŠ¨è¡¥å……ä¸­æ–‡æ–‡æœ¬åå†åˆ†æã€‚
        """)
    else:
        # è¯é¢‘ç»Ÿè®¡ï¼ˆä¼˜åŒ–ï¼šå³ä½¿ä¸è¶³20ä¸ªè¯ä¹Ÿèƒ½å±•ç¤ºï¼‰
        word_count = Counter(words[:5000])
        top_n = word_count.most_common(min(20, len(word_count)))  # å–æœ€å°æ•°é‡ï¼Œé¿å…ç©ºå€¼
        
        # åˆ†æ å±•ç¤ºç»“æœ
        col1, col2 = st.columns(2)
        
        with col1:
            st.subheader(f"1. è¯é¢‘ç»Ÿè®¡ï¼ˆTOP{len(top_n)}ï¼‰")
            df = pd.DataFrame(top_n, columns=["è¯æ±‡", "å‡ºç°æ¬¡æ•°"])
            st.dataframe(df, use_container_width=True)
            
            # è¯é¢‘æŸ±çŠ¶å›¾ï¼ˆå®¹é”™ï¼šè‡³å°‘3ä¸ªè¯æ‰å±•ç¤ºå›¾è¡¨ï¼‰
            st.subheader("2. è¯é¢‘å¯è§†åŒ–")
            if len(top_n) >= 3:
                try:
                    fig, ax = plt.subplots(figsize=(10, 6))
                    top_10 = top_n[:min(10, len(top_n))]
                    ax.bar([w[0] for w in top_10], [w[1] for w in top_10], color="#1f77b4")
                    ax.set_xlabel("è¯æ±‡", fontsize=12)
                    ax.set_ylabel("å‡ºç°æ¬¡æ•°", fontsize=12)
                    ax.set_title(f"TOP{len(top_10)}è¯æ±‡è¯é¢‘åˆ†å¸ƒ", fontsize=14)
                    plt.xticks(rotation=45)
                    st.pyplot(fig)
                except Exception as e:
                    st.error(f"âš ï¸ å›¾è¡¨ç”Ÿæˆå¤±è´¥ï¼š{str(e)[:50]}")
            else:
                st.info("âš ï¸ æœ‰æ•ˆè¯æ±‡ä¸è¶³3ä¸ªï¼Œæ— æ³•ç”ŸæˆæŸ±çŠ¶å›¾")
        
        with col2:
            st.subheader("3. æ ¸å¿ƒå…³é”®è¯")
            top_keywords = min(8, len(top_n))
            if top_keywords > 0:
                keywords = [w[0] for w in top_n[:top_keywords]]
                st.markdown(f"**{', '.join(keywords)}**")
            else:
                st.info("âš ï¸ æ— æœ‰æ•ˆå…³é”®è¯")
            
            # è¯äº‘å±•ç¤ºï¼ˆå®¹é”™ï¼‰
            st.subheader("4. è¯äº‘å±•ç¤º")
            try:
                wordcloud = generate_wordcloud(words)
                fig2, ax2 = plt.subplots(figsize=(10, 6))
                ax2.imshow(wordcloud, interpolation="bilinear")
                ax2.axis("off")
                st.pyplot(fig2)
            except Exception as e:
                st.error(f"âš ï¸ è¯äº‘ç”Ÿæˆå¤±è´¥ï¼š{str(e)[:50]}")

# é¡µè„š
st.divider()
st.caption("""
âœ¨ è°ƒè¯•è¾…åŠ©ï¼šä¾§è¾¹æ å¯æŸ¥çœ‹æ–‡æœ¬é¢„å¤„ç†å…¨è¿‡ç¨‹ï¼Œä¾¿äºå®šä½åˆ†æå¤±è´¥åŸå› ï¼›
âœ¨ æ¨èæµ‹è¯•URLï¼šhttps://www.ruanyifeng.com/blog/2025/01/weekly-issue-268.html
""")
